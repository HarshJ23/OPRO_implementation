{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPRO Implementation for MMLU Computer Science Questions\n",
    "### Modified version of Google DeepMind's OPRO framework optimizing for both accuracy and token count efficiency\n",
    "#### paper link : https://arxiv.org/abs/2309.03409\n",
    "\n",
    "\n",
    "#### to understand the underlying concept in layman terms check this simple presentation prepared by me : [Presentation](https://docs.google.com/presentation/d/1aTT6bXf9I1mFAhU5kmMMWRcUwsygnGRaXZ65U29SpWM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas openai tqdm tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\python311\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python311\\lib\\site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python311\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python311\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python311\\lib\\site-packages (from groq) (2.7.2)\n",
      "Requirement already satisfied: sniffio in c:\\python311\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\python311\\lib\\site-packages (from groq) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.18.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizationConfig:\n",
    "    \"\"\"Configuration parameters for optimization process\"\"\"\n",
    "    max_steps: int = 150  # max optimization steps\n",
    "    solutions_per_step: int = 8  # sol. generated per step\n",
    "    max_history: int = 20  # max no. of previous solutions to keep\n",
    "    temperature: float = 1.0  \n",
    "    token_weight: float = 0.3  # weightage of token length for scoring\n",
    "    max_tokens: int = 150  # token limit\n",
    "\n",
    "@dataclass\n",
    "class Solution:\n",
    "    \"\"\"Structure to hold solution data\"\"\"\n",
    "    instruction: str\n",
    "    accuracy: float\n",
    "    token_count: int\n",
    "    combined_score: float = 0.0\n",
    "    \n",
    "    def calculate_score(self, token_weight: float, max_tokens: int):\n",
    "        \"\"\"Calculate combined score considering both accuracy and token efficiency\"\"\"\n",
    "        token_score = 1 - (self.token_count / max_tokens)\n",
    "        self.combined_score = (1 - token_weight) * self.accuracy + token_weight * token_score\n",
    "        return self.combined_score\n",
    "\n",
    "\n",
    "        # if token weight = 1 --> consider only token weight as metric for optimization\n",
    "        # if token weight = 0 --> consider only accuracy on dataset as metric for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Section : Token Management\n",
    "##### Implement functions for token counting and management\n",
    "\n",
    "\n",
    "class TokenManager:\n",
    "    def __init__(self):\n",
    "        self.encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in given text\"\"\"\n",
    "        return len(self.encoder.encode(text))\n",
    "    \n",
    "    def is_within_limit(self, text: str, max_tokens: int) -> bool:\n",
    "        \"\"\"Check if text is within token limit\"\"\"\n",
    "        return self.count_tokens(text) <= max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MMLU dataset handler\n",
    "\n",
    "class MMluDataHandler:\n",
    "    def __init__(self, data_path: str):\n",
    "        \"\"\"Initialize with path to MMLU CS data\"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "    def prepare_data(self, train_ratio: float = 0.2):\n",
    "        \"\"\"split data into train and test sets\"\"\"\n",
    "        mask = np.random.rand(len(self.data)) < train_ratio\n",
    "        self.train_data = self.data[mask]\n",
    "        self.test_data = self.data[~mask]\n",
    "        \n",
    "    def get_sample_questions(self, n: int, from_train: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Get n random questions from dataset\"\"\"\n",
    "        source = self.train_data if from_train else self.test_data\n",
    "        return source.sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scorer component\n",
    "\n",
    "class Scorer:\n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.token_manager = TokenManager()\n",
    "        \n",
    "    def evaluate_solution(self, instruction: str, questions: pd.DataFrame) -> Tuple[float, int]:\n",
    "        \"\"\"Evaluate solution's accuracy and token count\"\"\"\n",
    "        correct = 0\n",
    "        token_count = self.token_manager.count_tokens(instruction)\n",
    "        \n",
    "        for _, row in questions.iterrows():\n",
    "            prompt = self._create_evaluation_prompt(instruction, row)\n",
    "            response = self._get_model_response(prompt)\n",
    "            if self._is_correct_answer(response, row['answer']):\n",
    "                correct += 1\n",
    "                \n",
    "        accuracy = correct / len(questions)\n",
    "        return accuracy, token_count\n",
    "    \n",
    "    def _create_evaluation_prompt(self, instruction: str, question_data: pd.Series) -> str:\n",
    "        \"\"\"Create prompt for evaluation\"\"\"\n",
    "        return f\"{instruction}\\n\\nQuestion: {question_data['question']}\\nA) {question_data['A']}\\nB) {question_data['B']}\\nC) {question_data['C']}\\nD) {question_data['D']}\"\n",
    "    \n",
    "    def _get_model_response(self, prompt: str) -> str:\n",
    "        \"\"\"Get response from OpenAI API\"\"\"\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _is_correct_answer(self, response: str, correct_answer: str) -> bool:\n",
    "        \"\"\"Check if response matches correct answer\"\"\"\n",
    "        return correct_answer.upper() in response.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimization component - core logic behind optimization\n",
    "class OptimizerEngine:\n",
    "    def __init__(self, config: OptimizationConfig):\n",
    "        self.config = config\n",
    "        self.scorer = Scorer()\n",
    "        self.token_manager = TokenManager()\n",
    "        self.solutions_history: List[Solution] = []\n",
    "        \n",
    "    def create_meta_prompt(self, exemplars: pd.DataFrame) -> str:\n",
    "        \"\"\"Create meta-prompt for optimization\"\"\"\n",
    "        # Sort solutions by combined score\n",
    "        sorted_solutions = sorted(\n",
    "            self.solutions_history[-self.config.max_history:],\n",
    "            key=lambda x: x.combined_score,\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Create prompt with previous solutions\n",
    "        solutions_text = \"\\n\".join([\n",
    "            f\"instruction: {sol.instruction}\\naccuracy: {sol.accuracy:.2f}\\ntokens: {sol.token_count}\\nscore: {sol.combined_score:.2f}\"\n",
    "            for sol in sorted_solutions\n",
    "        ])\n",
    "        \n",
    "        # Add exemplars\n",
    "        exemplars_text = \"\\n\\n\".join([\n",
    "            f\"Example {i+1}:\\n{row['question']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nCorrect: {row['answer']}\"\n",
    "            for i, (_, row) in enumerate(exemplars.iterrows())\n",
    "        ])\n",
    "        \n",
    "        return f\"\"\"You are an AI instruction optimizer. Create a new instruction for answering computer science questions that:\n",
    "1. Maximizes accuracy in answering questions\n",
    "2. Uses minimal number of tokens (be concise but effective)\n",
    "3. Is different from previous instructions\n",
    "\n",
    "Previous solutions (sorted by combined score):\n",
    "{solutions_text}\n",
    "\n",
    "Example questions:\n",
    "{exemplars_text}\n",
    "\n",
    "Generate a new instruction that should perform better than previous ones while being concise.\n",
    "Instruction should be specific to computer science domain and help in answering multiple-choice questions.\n",
    "\"\"\"\n",
    "\n",
    "    def generate_solutions(self, meta_prompt: str) -> List[str]:\n",
    "        \"\"\"Generate new candidate solutions\"\"\"\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": meta_prompt}],\n",
    "                temperature=self.config.temperature,\n",
    "                n=self.config.solutions_per_step\n",
    "            )\n",
    "            return [choice.message.content.strip() for choice in response.choices]\n",
    "        except Exception as e:\n",
    "            print(f\"Generation Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def optimize(self, data_handler: MMluDataHandler, num_steps: int) -> Dict:\n",
    "        \"\"\"Run optimization process\"\"\"\n",
    "        optimization_results = {\n",
    "            \"steps\": [],\n",
    "            \"best_solution\": None,\n",
    "            \"best_score\": 0\n",
    "        }\n",
    "        \n",
    "        for step in tqdm(range(num_steps)):\n",
    "            # get sample questions for evaluation\n",
    "            eval_questions = data_handler.get_sample_questions(3)\n",
    "            \n",
    "            # meta-prompt to generate solutions\n",
    "            meta_prompt = self.create_meta_prompt(eval_questions)\n",
    "            new_solutions = self.generate_solutions(meta_prompt)\n",
    "            \n",
    "            # Evaluation fo new soltions\n",
    "            for instruction in new_solutions:\n",
    "                if not self.token_manager.is_within_limit(instruction, self.config.max_tokens):\n",
    "                    continue\n",
    "                    \n",
    "                accuracy, token_count = self.scorer.evaluate_solution(instruction, eval_questions)\n",
    "                solution = Solution(instruction, accuracy, token_count)\n",
    "                solution.calculate_score(self.config.token_weight, self.config.max_tokens)\n",
    "                \n",
    "                self.solutions_history.append(solution)\n",
    "                \n",
    "                if solution.combined_score > optimization_results[\"best_score\"]:\n",
    "                    optimization_results[\"best_score\"] = solution.combined_score\n",
    "                    optimization_results[\"best_solution\"] = solution\n",
    "            \n",
    "            # step results\n",
    "            step_results = {\n",
    "                \"step\": step,\n",
    "                \"best_score\": optimization_results[\"best_score\"],\n",
    "                \"avg_score\": np.mean([s.combined_score for s in self.solutions_history[-self.config.solutions_per_step:]])\n",
    "            }\n",
    "            optimization_results[\"steps\"].append(step_results)\n",
    "            \n",
    "        return optimization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Init configuration\n",
    "    config = OptimizationConfig()\n",
    "    \n",
    "    # Setup data handler\n",
    "    data_handler = MMluDataHandler(\"path_to_mmlu_cs_data.csv\")\n",
    "    data_handler.prepare_data()\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = OptimizerEngine(config)\n",
    "    results = optimizer.optimize(data_handler, config.max_steps)\n",
    "    \n",
    "    # results timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(f\"optimization_results_{timestamp}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "        # display results\n",
    "    best_solution = results[\"best_solution\"]\n",
    "    print(f\"\\nBest Solution Found:\")\n",
    "    print(f\"Instruction: {best_solution.instruction}\")\n",
    "    print(f\"Accuracy: {best_solution.accuracy:.2f}\")\n",
    "    print(f\"Token Count: {best_solution.token_count}\")\n",
    "    print(f\"Combined Score: {best_solution.combined_score:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
